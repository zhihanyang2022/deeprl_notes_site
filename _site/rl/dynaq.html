<html>
<head>
	<meta charset="UTF-8">
	<title>Dyna-Q</title>
	<link rel="stylesheet" type="text/css" href="/assets/rl_post.css">
	<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
</head>
<body>
	<div class="content">
		<p><a href="/rl">Back</a> to the list of RL posts.</a>
		<h1>Dyna-Q</h1>
		<p>By Zhihan on 2020-08-11 15:30:00 -0500.</p>
		<p>Please note that this webpage does not scale well on mobile devices.</p>
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul id="markdown-toc">
  <li><a href="#what-is-planning" id="markdown-toc-what-is-planning">What is planning?</a></li>
  <li><a href="#the-tabular-one-step-dyna-q-algorithm" id="markdown-toc-the-tabular-one-step-dyna-q-algorithm">The tabular one-step Dyna-Q algorithm</a></li>
  <li><a href="#the-effect-of-planning-ratio-on-policy-learning" id="markdown-toc-the-effect-of-planning-ratio-on-policy-learning">The effect of planning ratio on policy learning</a></li>
  <li><a href="#when-the-model-is-wrong-dyna-q" id="markdown-toc-when-the-model-is-wrong-dyna-q">When the model is wrong: Dyna-Q+</a></li>
</ul>
<p>In this post, we learn the basic definition of planning in the context of reinforcement learning, and implement a simple algorithm called Dyna that adds planning to the one-step Q-learning algorithm.</p>

<h2 id="what-is-planning">What is planning?</h2>

<p>In RL, <strong>planning</strong> (formally known as <em>state-space plannning</em>) is the evaluation the value function under a policy using simulated experiences generated by the model of an environment (explained shortly). This sounds a lot like <strong>learning</strong>, the evaluation of the value function under a policy using real experiences. Therefore, they serve the same purpose: in both cases, making the policy greedy / epsilon-greedy with respect to a more accurate value function improves that policy.</p>

<p>A <strong>model</strong> of the environment is anything that an agent can use to predict how the environment will respond to its  actions. There are two types of models:</p>

<ul>
  <li>Distribution models: given a state-action pair, produce all possible next states and their probabilities</li>
  <li>Sample models: given a state-action pair, produce one possible next state</li>
</ul>

<p>Distribution models are stronger than sample models because they can do everything a sample model can; however, they are generally harder to obtain for many problems.</p>

<p>Here’s a flowchart of how planning works:
<script type="math/tex">\begin{align*}
\text{model (input)} \to \text{simulated experiences} \to \text{values} \to \text{improved policy (output)}
\end{align*}</script>
Here possible relationships between experience, model, values, and policy are summarized below:</p>

<p><img src="https://i.loli.net/2020/08/12/wUje1WShKVT8csl.png" alt="image-20200811155226803" width="300" /></p>

<p>From the diagram above, we that there are at least two roles for real experience:</p>

<ul>
  <li>Improve the value function and thus policy using the learning methods discussed in previous chapters (via direct RL)</li>
  <li>Improve the model (via model learning)</li>
</ul>

<p>Note that, in both ways (one direct and one indirect), the value functions and policies are improved. Therefore, these two approaches are also called <strong>direct RL</strong> and <strong>indirect RL</strong>.</p>

<p>At this point, we don’t attempt to say whether learning or planning is better - they have deep connections. For example, my other post on TD control emphasizes the link between TD control (a model-free method) and value iteration (a model-based method).</p>

<h2 id="the-tabular-one-step-dyna-q-algorithm">The tabular one-step Dyna-Q algorithm</h2>

<p>For illustration purposes, the following version of the algorithm assumes that the environment is deterministic in terms of next states and rewards. If the code between <code class="highlighter-rouge">planning: start</code> and <code class="highlighter-rouge">planning: end</code> is removed (or if <code class="highlighter-rouge">n</code> is set to zero), then we would have the Q-learning algorithm.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize Q(s, a) for all (s, a) pairs with Q(terminal, .) = 0.
Initialize Model to be an empty dictionary.
Initialize VisitedStates to be an empty set.
Initialize ActionsInState to be an empty dictionary.

Set max_episodes.
Set alpha - the learning rate.
Set gamma - the discount factor.
Set n - the planning-learning ratio.

for episode in range(max_episodes):

    Initialize s to be the starting state.
    
    Loop:
        
        # ===== learning: start =====
        
        Choose a using s from the epsilon-greedy (behavior) policy derived from Q.
        Take action a, observe r and s'.
        
        # preparation for planning
        
        VisitedStates.add(s)
        if s in ActionsInState.keys():
            ActionsInState[s].append(s)
        else:
            ActionsInState[s] = [a]
        Model[(s, a)] = (r, s')  
            
        Bellman sample = r + gamma * max_a Q(s', a)
        
        Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample

        s = s'
        
        # ===== learning: end =====
        
        # ===== planning: start =====
        
        for _ in range(n):
            
            s = randomly chosen state from VisitedStates
            a = random action chosen from ActionsInState[s]
            r, s' = Model[(s, a)]
            
            Bellman sample = r + gamma * max_a Q(s', a)
            
            Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample
            
        # ===== planning: end =====
        			
    until s is terminal.
</code></pre></div></div>

<h2 id="the-effect-of-planning-ratio-on-policy-learning">The effect of planning ratio on policy learning</h2>

<p><img src="/images/2020-08-11-dynaq/dynaq_online_performance.png" width="700" /></p>

<div class="two-images-container">
  <div class="image-container">
    <img src="/images/2020-08-11-dynaq/dynaq_n20_after_first_episode_policy.png" class="image" />
    <p align="center">Policy learned after first episode for a planning ratio of 20.</p>
  </div>
  <div class="image-container">
    <img src="/images/2020-08-11-dynaq/dynaq_n20_after_second_episode_policy.png" class="image" />
    <p align="center">Policy learned after second episode for a planning ratio of 20.</p>
  </div>
</div>

<h2 id="when-the-model-is-wrong-dyna-q">When the model is wrong: Dyna-Q+</h2>

<p>All modifications (with respect to Dyna-Q) are marked by <code class="highlighter-rouge">**********</code>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize Q(s, a) for all (s, a) pairs with Q(terminal, .) = 0.
Initialize Tau(s, a) for all (s, a) pairs to be zero.  # **********
Initialize Model(s, a) for all (s, a) pairs to be (0, s).  # **********
Initialize VisitedStates to be an empty set.
Initialize ActionsInState to be an empty dictionary.

Set max_episodes.
Set alpha - the learning rate.
Set gamma - the discount factor.
Set n - the planning-learning ratio.
Set k - the exploration constant.

for episode in range(max_episodes):

    Initialize s to be the starting state.
    
    Loop:
        
        # **********
        for (s, a) in S cross A:
            Tau(s, a) += 1
        
        # ===== learning: start =====
        
        Choose a using s from the epsilon-greedy (behavior) policy derived from Q.
        Take action a, observe r and s'.
        
        # preparation for planning
        
        Tau(s, a) = 0  # **********
        
        VisitedStates.add(s)
        if s in ActionsInState.keys():
            ActionsInState[s].append(s)
        else:
            ActionsInState[s] = [a]
        Model[(s, a)] = (r, s')  
            
        Bellman sample = r + gamma * max_a Q(s', a)
        
        Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample

        s = s'
        
        # ===== learning: end =====
        
        # ===== planning: start =====
        
        for _ in range(n):
            
            s = randomly chosen state from VisitedStates
            a = random action chosen from ActionsInState[s]
            r, s' = Model[(s, a)]
            
            exploration bonus = k * sqrt(Tau(s, a))  # **********
            Bellman sample = (r + bonus) + gamma * max_a Q(s', a)  # **********
            
            Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample
            
        # ===== planning: end =====
        			
    until s is terminal.
</code></pre></div></div>


		<p><a href="/rl">Back</a> to the list of RL posts.</a>
	</div>
</body>
</html>