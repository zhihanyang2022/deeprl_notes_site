---
layout: rl_post
title:  Dyna-Q
date:   2020-08-11 15:30:00 -0500
permalink: /rl/dynaq
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

- toc
{:toc}
In this post, we learn the basic definition of planning in the context of reinforcement learning, and implement a simple algorithm called Dyna that adds planning to the one-step Q-learning algorithm.

## What is planning?

In RL, **planning** (formally known as *state-space plannning*) is the evaluation the value function under a policy using simulated experiences generated by the model of an environment (explained shortly). This sounds a lot like **learning**, the evaluation of the value function under a policy using real experiences. Therefore, they serve the same purpose: in both cases, making the policy greedy / epsilon-greedy with respect to a more accurate value function improves that policy. 

A **model** of the environment is anything that an agent can use to predict how the environment will respond to its  actions. There are two types of models:

- Distribution models: given a state-action pair, produce all possible next states and their probabilities
- Sample models: given a state-action pair, produce one possible next state

Distribution models are stronger than sample models because they can do everything a sample model can; however, they are generally harder to obtain for many problems.

Here’s a flowchart of how planning works:
$$
\begin{align*}
\text{model (input)} \to \text{simulated experiences} \to \text{values} \to \text{improved policy (output)}
\end{align*}
$$
Here possible relationships between experience, model, values, and policy are summarized below:

<img src="https://i.loli.net/2020/08/12/wUje1WShKVT8csl.png" alt="image-20200811155226803" width="300"/>

From the diagram above, we that there are at least two roles for real experience:

- Improve the value function and thus policy using the learning methods discussed in previous chapters (via direct RL)
- Improve the model (via model learning)

Note that, in both ways (one direct and one indirect), the value functions and policies are improved. Therefore, these two approaches are also called **direct RL** and **indirect RL**.

At this point, we don’t attempt to say whether learning or planning is better - they have deep connections. For example, my other post on TD control emphasizes the link between TD control (a model-free method) and value iteration (a model-based method).

## The tabular one-step Dyna-Q algorithm

For illustration purposes, the following version of the algorithm assumes that the environment is deterministic in terms of next states and rewards. If the code between `planning: start` and `planning: end` is removed (or if `n` is set to zero), then we would have the Q-learning algorithm.

```
Initialize Q(s, a) for all (s, a) pairs with Q(terminal, .) = 0.
Initialize Model to be an empty dictionary.
Initialize VisitedStates to be an empty set.
Initialize ActionsInState to be an empty dictionary.

Set max_episodes.
Set alpha - the learning rate.
Set gamma - the discount factor.
Set n - the planning-learning ratio.

for episode in range(max_episodes):

    Initialize s to be the starting state.
    
    Loop:
        
        # ===== learning: start =====
        
        Choose a using s from the epsilon-greedy (behavior) policy derived from Q.
        Take action a, observe r and s'.
        
        # preparation for planning
        
        VisitedStates.add(s)
        if s in ActionsInState.keys():
            ActionsInState[s].append(s)
        else:
            ActionsInState[s] = [a]
        Model[(s, a)] = (r, s')  
            
        Bellman sample = r + gamma * max_a Q(s', a)
        
        Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample

        s = s'
        
        # ===== learning: end =====
        
        # ===== planning: start =====
        
        for _ in range(n):
            
            s = randomly chosen state from VisitedStates
            a = random action chosen from ActionsInState[s]
            r, s' = Model[(s, a)]
            
            Bellman sample = r + gamma * max_a Q(s', a)
            
            Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample
            
        # ===== planning: end =====
        			
    until s is terminal.
```

## The effect of planning ratio on policy learning



<img src="/images/2020-08-11-dynaq/dynaq_online_performance.png" width="700">

<div class="two-images-container">
  <div class="image-container">
    <img src='/images/2020-08-11-dynaq/dynaq_n20_after_first_episode_policy.png' class="image">
    <p align='center'>Policy learned after first episode for a planning ratio of 20.</p>
  </div>
  <div class="image-container">
    <img src='/images/2020-08-11-dynaq/dynaq_n20_after_second_episode_policy.png' class="image">
    <p align='center'>Policy learned after second episode for a planning ratio of 20.</p>
  </div>
</div>


## When the model is wrong: Dyna-Q+



All modifications (with respect to Dyna-Q) are marked by `**********`.

```
Initialize Q(s, a) for all (s, a) pairs with Q(terminal, .) = 0.
Initialize Tau(s, a) for all (s, a) pairs to be zero.  # **********
Initialize Model(s, a) for all (s, a) pairs to be (0, s).  # **********
Initialize VisitedStates to be an empty set.
Initialize ActionsInState to be an empty dictionary.

Set max_episodes.
Set alpha - the learning rate.
Set gamma - the discount factor.
Set n - the planning-learning ratio.
Set k - the exploration constant.

for episode in range(max_episodes):

    Initialize s to be the starting state.
    
    Loop:
        
        # **********
        for (s, a) in S cross A:
            Tau(s, a) += 1
        
        # ===== learning: start =====
        
        Choose a using s from the epsilon-greedy (behavior) policy derived from Q.
        Take action a, observe r and s'.
        
        # preparation for planning
        
        Tau(s, a) = 0  # **********
        
        VisitedStates.add(s)
        if s in ActionsInState.keys():
            ActionsInState[s].append(s)
        else:
            ActionsInState[s] = [a]
        Model[(s, a)] = (r, s')  
            
        Bellman sample = r + gamma * max_a Q(s', a)
        
        Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample

        s = s'
        
        # ===== learning: end =====
        
        # ===== planning: start =====
        
        for _ in range(n):
            
            s = randomly chosen state from VisitedStates
            a = random action chosen from ActionsInState[s]
            r, s' = Model[(s, a)]
            
            exploration bonus = k * sqrt(Tau(s, a))  # **********
            Bellman sample = (r + bonus) + gamma * max_a Q(s', a)  # **********
            
            Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample
            
        # ===== planning: end =====
        			
    until s is terminal.
```



